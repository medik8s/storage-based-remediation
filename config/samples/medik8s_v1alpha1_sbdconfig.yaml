# SBDConfig Sample Configuration
#
# This is a sample configuration for the SBD (STONITH Block Device) operator.
# The SBD operator provides watchdog-based fencing for Kubernetes clusters
# to ensure high availability through automatic node remediation.
#
# For comprehensive documentation, see: docs/sbdconfig-user-guide.md

apiVersion: medik8s.medik8s.io/v1alpha1
kind: SBDConfig
metadata:
  labels:
    app.kubernetes.io/name: sbd-operator
    app.kubernetes.io/managed-by: kustomize
  name: sbdconfig-sample
spec:
  # ========================================================================
  # WATCHDOG CONFIGURATION
  # ========================================================================
  
  # Path to the watchdog device on cluster nodes
  # - Default: "/dev/watchdog"
  # - Common alternatives: "/dev/watchdog0", "/dev/watchdog1"
  # - If hardware watchdog is not available, softdog will be used automatically
  # - All nodes should have the same watchdog device path
  # sbdWatchdogPath: "/dev/watchdog"
  
  # Watchdog timeout configuration for hardware/software watchdog device
  # - Default: "60s" (60 seconds)
  # - Range: "10s" to "300s" (5 minutes)
  # - Format: Go duration (e.g., "30s", "90s", "2m")
  # - This determines how long the system waits before triggering a reboot
  # - Hardware watchdogs may have device-specific limits
  # - Shorter timeouts: faster failure detection, more aggressive
  # - Longer timeouts: more conservative, handles temporary delays
  # 
  # Recommended values:
  # - Development/Testing: "30s" - "60s"
  # - Production (fast response): "60s" - "120s"
  # - Production (conservative): "120s" - "180s"
  watchdogTimeout: "60s"
  
  # Pet interval multiple for calculating watchdog pet frequency
  # - Default: 4 (pet interval = watchdog timeout / 4)
  # - Range: 3 to 20
  # - Higher values: less frequent petting, lower overhead
  # - Lower values: more frequent petting, higher safety margin
  # - Pet interval must be at least 3x shorter than watchdog timeout
  # 
  # With default 60s watchdog timeout:
  # - Multiple 3: pet every 20s (minimum safe)
  # - Multiple 4: pet every 15s (recommended)
  # - Multiple 6: pet every 10s (conservative)
  # - Multiple 10: pet every 6s (very conservative)
  petIntervalMultiple: 4
  
  # ========================================================================
  # CONTAINER IMAGE CONFIGURATION
  # ========================================================================
  
  # Container image for the SBD Agent DaemonSet
  # - If not specified, automatically derived from the operator's image
  #   (same registry/org/tag as operator, but with "sbd-agent" as image name)
  # - Use specific version tags for production deployments
  # - Available images: quay.io/medik8s/sbd-agent:<version>
  # - For development: use "latest" tag
  # - For production: use specific version like "v1.0.0"
  # 
  # Examples:
  # - If operator image is "quay.io/medik8s/sbd-operator:v1.2.3"
  #   then agent image becomes "quay.io/medik8s/sbd-agent:v1.2.3"
  # - Override with custom image if needed:
  # image: "quay.io/medik8s/sbd-agent:latest"
  
  # Image pull policy for the SBD Agent container
  # - Always: Always pull the image, useful for development with "latest" tags
  # - IfNotPresent: Pull only if image not present locally (default, production-safe)
  # - Never: Never pull, only use local images
  # Default: "IfNotPresent" for production stability
  imagePullPolicy: "IfNotPresent"
  
  # ========================================================================
  # DEPLOYMENT CONFIGURATION
  # ========================================================================
  
  # Node selector for controlling which nodes the SBD Agent runs on
  # - Default: worker nodes only (node-role.kubernetes.io/worker: "")
  # - Automatically merged with Linux OS requirement (kubernetes.io/os: linux)
  # - Customize to target specific nodes by labels
  # 
  # Examples:
  # - Default behavior (worker nodes only) - no configuration needed
  # - All nodes: nodeSelector: {}
  # - Specific zone: nodeSelector: {"topology.kubernetes.io/zone": "us-east-1a"}
  # - Custom labels: nodeSelector: {"sbd-enabled": "true", "tier": "compute"}
  # 
  # Note: The controller automatically adds "kubernetes.io/os: linux" requirement
  # The SBD Agent DaemonSet will be deployed in the same namespace as this SBDConfig resource
  # nodeSelector:
  #   node-role.kubernetes.io/worker: ""
  
  # ========================================================================
  # NODE LIFECYCLE CONFIGURATION
  # ========================================================================
  
  # Timeout for considering nodes stale and cleaning up their slot assignments
  # - Default: "1h" (1 hour)
  # - Range: "1m" to "24h"
  # - Format: Go duration (e.g., "30m", "2h", "90s")
  # - Shorter timeouts: faster slot cleanup, more aggressive
  # - Longer timeouts: more conservative, handles temporary network issues
  # 
  # Recommended values:
  # - Development/Testing: "5m" - "15m"
  # - Production (stable): "30m" - "2h"  
  # - Production (critical): "2h" - "6h"
  staleNodeTimeout: "1h"
  
  # ========================================================================
  # SHARED STORAGE CONFIGURATION (OPTIONAL)
  # ========================================================================
  
  # Storage class name for creating shared storage for cross-node coordination
  # - When specified, the controller creates a PVC using this StorageClass
  # - The PVC is mounted in all SBD agent pods for coordination and slot assignment
  # - The StorageClass must support ReadWriteMany (RWX) access mode
  # - Common RWX storage classes: "efs-sc", "nfs-client", "cephfs", "glusterfs"
  # 
  # Examples:
  # - AWS EFS: "efs-sc"
  # - NFS: "nfs-client" 
  # - CephFS: "cephfs"
  # - GlusterFS: "glusterfs"
  # 
  # Note: If not specified, SBD agents will operate independently without coordination
  # sharedStorageClass: "efs-sc"

# ========================================================================
# MULTIPLE SBDCONFIG SUPPORT (NEW in v1.1+)
# ========================================================================
#
# The SBD operator now supports multiple SBDConfig resources in the same namespace!
# This enables advanced deployment scenarios like A/B testing, gradual rollouts,
# and different configurations for different teams.
#
# KEY FEATURES:
# - Shared service account across all SBDConfigs in a namespace
# - Separate DaemonSets with unique names for each SBDConfig
# - Independent configurations (images, timeouts, node selectors, etc.)
# - Automatic cleanup when SBDConfigs are deleted
#
# RESOURCE NAMING:
# - Service Account: sbd-agent (shared across all SBDConfigs in namespace)
# - DaemonSet: sbd-agent-{sbdconfig-name} (unique per SBDConfig)
# - ClusterRoleBinding: sbd-agent-{namespace}-{sbdconfig-name} (globally unique)
#
# EXAMPLE USAGE:
# kubectl apply -f production-sbd.yaml -f canary-sbd.yaml
# kubectl get sbdconfig -n my-app
# kubectl get daemonset -n my-app -l app=sbd-agent
#
# ========================================================================
# CONFIGURATION EXAMPLES
# ========================================================================
#
# Basic Production Configuration (image auto-derived from operator):
# -------------------------------------------------------------------
# spec:
#   staleNodeTimeout: "30m"
#
# Production with Custom Image:
# -----------------------------
# spec:
#   image: "quay.io/medik8s/sbd-agent:latest"
#   staleNodeTimeout: "30m"
#
# Multiple Configurations in Same Namespace:
# ------------------------------------------
# # Production config
# metadata:
#   name: production-sbd
#   namespace: my-app
# spec:
#   image: "quay.io/medik8s/sbd-agent:latest"
#   imagePullPolicy: "IfNotPresent"
# ---
# # Canary config  
# metadata:
#   name: canary-sbd
#   namespace: my-app
# spec:
#   image: "quay.io/medik8s/sbd-agent:latest"
#   imagePullPolicy: "Always"
#   nodeSelector:
#     canary: "true"
#
# High-Availability Configuration:
# --------------------------------
# spec:
#   sbdWatchdogPath: "/dev/watchdog1"
#   staleNodeTimeout: "2h"
#
# Development/Testing Configuration:
# ----------------------------------
# spec:
#   imagePullPolicy: "Always"
#   staleNodeTimeout: "5m"
#
# Note: Each SBDConfig creates its own DaemonSet in the specified namespace.
# Multiple SBDConfigs in the same namespace share the service account but
# have separate DaemonSets and configurations.
#
# ========================================================================
# DEPLOYMENT COMMANDS
# ========================================================================
#
# Apply configuration:
#   kubectl apply -f medik8s_v1alpha1_sbdconfig.yaml
#
# Check status:
#   kubectl get sbdconfig
#   kubectl describe sbdconfig sbdconfig-sample
#
# Monitor deployment:
#   kubectl get pods -n <namespace>
#   kubectl logs -n <namespace> -l app=sbd-agent
#   (Replace <namespace> with the namespace where you created the SBDConfig)
#
# ========================================================================
# OPENSHIFT SECURITY CONTEXT CONSTRAINTS (SCC) MANAGEMENT
# ========================================================================
#
# The SBD operator automatically manages OpenShift SecurityContextConstraints
# when deployed on OpenShift clusters. The controller will:
#
# 1. Detect if running on OpenShift platform
# 2. Check for the required SCC: sbd-operator-sbd-agent-privileged
# 3. Automatically update the SCC to grant permissions to service accounts
#    in any namespace where SBDConfig resources are deployed
#
# Example: If you create an SBDConfig in namespace "my-app", the operator
# will automatically add "system:serviceaccount:my-app:sbd-agent" to the
# SCC users list, allowing pods to start with required privileges.
#
# Manual SCC management (if needed):
#   # Check SCC status
#   oc get scc sbd-operator-sbd-agent-privileged -o yaml
#   
#   # Manually add service account (normally automatic)
#   oc adm policy add-scc-to-user sbd-operator-sbd-agent-privileged \
#     system:serviceaccount:my-namespace:sbd-agent
#
# Note: The base SCC must be installed via the OpenShift installer:
#   make build-openshift-installer
#   kubectl apply -f dist/install-openshift.yaml
#
# ========================================================================