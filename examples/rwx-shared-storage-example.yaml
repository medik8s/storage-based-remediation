---
# Example: Using RWX Storage Class for SBD Shared Storage
# This example shows how to use a StorageClass for automatic shared storage
# provisioning for SBD agent coordination across multiple nodes

apiVersion: v1
kind: ConfigMap
metadata:
  name: sbd-shared-config
  namespace: sbd-system
data:
  shared-data.txt: |
    This is shared configuration data that can be accessed
    by all SBD agent pods across different worker nodes.
    
    Use cases:
    - Shared SBD device metadata
    - Coordination data between agents
    - Shared logs or status files
    - Cross-node communication files

---
# SBDConfig with RWX shared storage integration
# This demonstrates the native shared storage support in SBDConfig CRD
apiVersion: medik8s.medik8s.io/v1alpha1
kind: SBDConfig
metadata:
  name: sbd-config-with-shared-storage
  namespace: sbd-system
spec:
  # SBD agent image configuration
  image: "quay.io/medik8s/sbd-agent:latest"
  imagePullPolicy: "IfNotPresent"
  
  # Watchdog configuration
  sbdWatchdogPath: "/dev/watchdog"
  watchdogTimeout: "60s"
  petIntervalMultiple: 4
  
  # Node management
  staleNodeTimeout: "1h"
  
  # ✅ SHARED STORAGE CONFIGURATION ✅
  # This field enables automatic shared storage integration
  # The controller will create a PVC using this StorageClass and configure the sbd-agent accordingly
  sharedStorageClass: "efs-sc"                 # Required: Name of RWX StorageClass
  sharedStorageMountPath: "/sbd-block"         # Optional: Defaults to /sbd-block
  
  # When shared storage is configured, the controller automatically:
  # 1. Creates a PVC using the specified StorageClass with RWX access mode
  # 2. Validates the StorageClass name and mount path
  # 3. Adds the PVC as a volume in the DaemonSet
  # 4. Mounts it at the specified path in sbd-agent containers
  # 5. Passes --shared-storage=/sbd-block to the sbd-agent command
  # 6. Enables cross-node coordination features
  #
  # Common StorageClass names:
  # - AWS EFS: "efs-sc"
  # - NFS: "nfs-client"
  # - CephFS: "cephfs"
  # - GlusterFS: "glusterfs"

---
# Example Pod to test RWX storage functionality
# This pod will use the auto-generated PVC created by the SBDConfig controller
apiVersion: v1
kind: Pod
metadata:
  name: rwx-storage-test
  namespace: sbd-system
  labels:
    app: storage-test
spec:
  containers:
  - name: writer
    image: registry.access.redhat.com/ubi9/ubi-minimal:latest
    command:
    - /bin/bash
    - -c
    - |
      echo "Testing RWX storage from pod on node: $NODE_NAME"
      
      # Create test directory
      mkdir -p /shared/test-data
      
      # Write test data with timestamp and node info
      while true; do
        echo "$(date): Hello from $NODE_NAME" >> /shared/test-data/messages.log
        echo "Current files in shared storage:"
        find /shared -type f | head -10
        sleep 30
      done
    
    env:
    - name: NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
    
    volumeMounts:
    - name: shared-storage
      mountPath: /shared
  
  volumes:
  - name: shared-storage
    persistentVolumeClaim:
      # Use the auto-generated PVC name: {sbdconfig-name}-shared-storage
      claimName: sbd-config-with-shared-storage-shared-storage
  
  restartPolicy: Always

---
# Example Job to verify RWX functionality
apiVersion: batch/v1
kind: Job
metadata:
  name: verify-rwx-storage
  namespace: sbd-system
spec:
  template:
    spec:
      containers:
      - name: verifier
        image: registry.access.redhat.com/ubi9/ubi-minimal:latest
        command:
        - /bin/bash
        - -c
        - |
          echo "=== RWX Storage Verification ==="
          echo "Node: $NODE_NAME"
          echo "Shared storage contents:"
          
          # List contents
          find /shared -type f -exec ls -la {} \; | head -20
          
          # Test write access
          echo "Testing write access..."
          test_file="/shared/verification-$(date +%s)-$NODE_NAME.txt"
          echo "Verification test from $NODE_NAME at $(date)" > "$test_file"
          
          if [[ -f "$test_file" ]]; then
            echo "✅ Write test successful: $test_file"
          else
            echo "❌ Write test failed"
            exit 1
          fi
          
          # Test read access to files from other nodes
          echo "Testing read access to files from other nodes..."
          other_files=$(find /shared -name "verification-*" ! -name "*$NODE_NAME*" | wc -l)
          echo "Found $other_files files from other nodes"
          
          echo "=== Verification Complete ==="
        
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        
        volumeMounts:
        - name: shared-storage
          mountPath: /shared
      
      volumes:
      - name: shared-storage
        persistentVolumeClaim:
          # Use the auto-generated PVC name: {sbdconfig-name}-shared-storage
          claimName: sbd-config-with-shared-storage-shared-storage
      
      restartPolicy: Never
  backoffLimit: 3 